# Physical AI — Chat‑Controlled Robot Simulator (MVP)

**Goal:** 대화(자연어)로 로봇을 조작하는 간단한 시뮬레이터 MVP.

* 사용자는 "오른팔 올려" / "rotate right elbow 30 deg" 처럼 말하거나 타이핑하면, 3D 시뮬레이션(react‑three‑fiber) 화면의 로봇이 해당 동작을 수행합니다.
* 초기 버전은 **규칙 기반 파서**로 시작하고, 추후 **GPT**로 Intent를 추출합니다.
* 코드는 Claude Code를 활용해 보일러플레이트/컴포넌트 생성을 가속합니다.

---

## 1) 아키텍처 개요

```
App (UI Shell)
 ├─ Scene (3D View: @react-three/fiber, drei, OrbitControls)
 │   └─ Humanoid (로봇 1대) × N
 │       └─ Shoulder/Elbow Joint (회전값에 따라 애니메이션)
 ├─ ChatPanel (명령 입력/프리셋/로그)
 ├─ Parser (규칙 기반 → 나중에 GPT 의사결정으로 교체)
 └─ State (robots[], selectedId, pose targets)
```

* **Intent Schema(핵심)**

```json
{
  "type": "pose|delta|wave|reset|noop|unknown",
  "side": "left|right",
  "joint": "shoulder|elbow",
  "axis": "pitch|flex",
  "angle": 0,
  "delta": 0,
  "text": "raw input"
}
```

> MVP에서는 `shoulder.pitch`(0–120°)와 `elbow.flex`(0–140°)만 제어합니다. 안전을 위해 각도를 클램핑합니다.

---

## 2) 기술 스택

* **Frontend:** React + Vite 또는 Next.js (CSR 우선), TypeScript 권장
* **3D:** react-three-fiber, @react-three/drei
* **Motion:** Framer Motion
* **Styles:** Tailwind CSS
* **(선택) 음성입력:** Web Speech API
* **LLM 파서:** OpenAI GPT (function calling), 이후 확장

---

## 3) 빠른 시작 (로컬)

### A. 프로젝트 생성

* Vite(권장):

```bash
npm create vite@latest physical-ai-sim -- --template react-ts
cd physical-ai-sim
npm install
```

* 필수 라이브러리 설치:

```bash
npm i three @react-three/fiber @react-three/drei framer-motion
npm i -D tailwindcss postcss autoprefixer
npx tailwindcss init -p
```

* `tailwind.config.js` / `index.css` 기본 세팅 추가.

### B. 컴포넌트 배치

* `src/components/PhysicalAIMVP.tsx` 파일 생성 → (캔버스에 올려둔 컴포넌트 코드 붙여넣기)
* `src/App.tsx`에서 컴포넌트를 렌더링:

```tsx
import PhysicalAIMVP from "./components/PhysicalAIMVP";
export default function App(){
  return <div className="h-screen"><PhysicalAIMVP/></div>;
}
```

### C. 실행

```bash
npm run dev
```

→ 브라우저에서 `http://localhost:5173` (Vite 기본 포트) 접속.

---

## 4) 명령 문법(현재 버전)

* `raise/lower <left|right> arm`
* `rotate <left|right> shoulder|elbow <angle> deg`
* `wave <left|right> arm`
* `reset`

> 한국어 예시: `오른팔 올려` → 초기엔 규칙 매칭으로 `raise right arm`에 맵핑하여 처리.

---

## 5) GPT 연동 (Intent 추출)

### A. 전략

1. 사용자 입력(raw text)을 백엔드(또는 Edge 함수)로 전달
2. OpenAI GPT에 **함수 호출 스펙**을 제공하여 structured intent를 받음
3. 프론트엔드에서는 해당 intent를 그대로 `applyAction(intent)`에 전달

### B. 함수 스펙 예시 (의사코드)

```json
{
  "name": "set_robot_motion_intent",
  "description": "Map a natural language command to a robot motion intent",
  "parameters": {
    "type": "object",
    "properties": {
      "type": {"type": "string", "enum": ["pose","delta","wave","reset","noop","unknown"]},
      "side": {"type": "string", "enum": ["left","right" ]},
      "joint": {"type": "string", "enum": ["shoulder","elbow"]},
      "axis": {"type": "string", "enum": ["pitch","flex"]},
      "angle": {"type": "number"},
      "delta": {"type": "number"},
      "text": {"type": "string"}
    },
    "required": ["type"]
  }
}
```

### C. 서버(예: Next.js API Route)

```ts
// /pages/api/intent.ts (또는 /app/api/intent/route.ts in Next 13+)
import type { NextApiRequest, NextApiResponse } from "next";
import OpenAI from "openai";

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

export default async function handler(req: NextApiRequest, res: NextApiResponse){
  const { text } = req.body;
  const tools = [/* 위 함수 스펙 */];
  const sys = "You convert natural language robot commands into a strict motion intent schema.";

  const r = await openai.chat.completions.create({
    model: "gpt-4o-mini", // 비용/지연 고려하여 선택
    messages: [
      { role: "system", content: sys },
      { role: "user", content: text }
    ],
    tools,
    tool_choice: { type: "function", function: { name: "set_robot_motion_intent" } }
  });

  const call = r.choices[0]?.message?.tool_calls?.[0];
  const intent = call ? JSON.parse(call.function.arguments) : { type: "unknown", text };
  res.json({ intent });
}
```

### D. 프론트엔드 연결

```ts
// handleSubmit에서 raw input을 /api/intent로 POST → intent 반환 → applyAction(intent)
```

> **주의:** 각도/속도/관절 범위는 프론트에서 **추가로 검증**(클램프)하여 안전성 확보.

---

## 6) Claude Code 활용 가이드

Claude Code(IDE 도우미)에게 아래와 같이 프롬프트하세요:

* **컴포넌트 분리**:

  * "`PhysicalAIMVP`를 `Scene`, `Humanoid`, `ChatPanel`, `useIntentParser` 훅으로 모듈화하고 타입 정의를 강화해줘. 각 파일의 역할과 prop 타입을 명확히."
* **반응형/스타일**:

  * "Tailwind로 모바일 레이아웃 최적화, 버튼/카드에 focus-visible 스타일 추가, 접근성 속성(aria-label 등) 보완해줘."
* **테스트**:

  * "Vitest + React Testing Library로 `parseCommand` 단위 테스트 8개 생성. 경계값(120°, 140°) 클램프 검사 포함."
* **성능**:

  * "로봇 N대일 때 useFrame 성능 최적화: joint 참조 배치, memoization, shallow 비교로 리렌더 최소화."

---

## 7) 로드맵

**Phase 1 — MVP (현재)**

* 규칙 기반 파서, 어깨/팔꿈치 제어, 다중 로봇, 프리셋, 의도 로그

**Phase 2 — LLM 파서 + 음성**

* GPT 함수호출 기반 Intent 추출
* Web Speech API 마이크 버튼 (한국어/영어 지원)

**Phase 3 — 관절 확장 + IK**

* 어깨 yaw/roll, 손목, 손가락
* IK (CCD/FABRIK)로 목표점 따라잡기 (`move right hand to x,y,z`)

**Phase 4 — 물리 시뮬레이션**

* cannon-es/Ammo.js로 조인트 제약, 토크 제한, 중력/충돌
* 안전 모드(속도 제한, 특정 포즈 금지)

**Phase 5 — 로봇 에디터 & 시나리오**

* 파츠 길이/관절범위/질량 설정, 프리셋 저장/불러오기
* 시나리오 스크립팅: `repeat wave 3 times`, `then reset`

---

## 8) 보안 & 안전

* **입력 검증:** LLM 반환 값에 대해 화이트리스트/스키마 검증(JSON Schema) 후 적용
* **물리 한계:** 관절 각도/속도/토크 하드 클램프
* **프롬프트 방어:** system 프롬프트에 범위를 명확히, 함수 파라미터 이외의 출력 무시
* **로그:** 사용자 커맨드/의도를 익명 로그로 저장(개인정보 배제)

---

## 9) 프로젝트 구조(예시)

```
src/
  components/
    PhysicalAIMVP.tsx
    Scene.tsx
    Humanoid.tsx
    ChatPanel.tsx
  hooks/
    useIntentParser.ts
  lib/
    types.ts
    clamp.ts
  pages/ (Next.js라면)
    api/intent.ts
  styles/
    index.css
```

---

## 10) 환경변수(.env)

```
OPENAI_API_KEY=sk-...
```

> 서버(Edge/Node)에서만 사용. 브라우저에 노출 금지.

---

## 11) 테스트(권장)

* **유닛:** `parseCommand` → 다양한 문장/오타/한국어 매핑 테스트
* **E2E:** Playwright로 기본 상호작용(로봇 추가, 선택, 명령 실행) 시나리오

---

## 12) 라이선스

* 개인 프로젝트 시작은 **MIT** 추천. 사내/상업용 전환 시 별도 검토.

---

## 13) FAQ

* **Q. 한국어 명령도 되나?**
  A. MVP는 간단 매핑으로 처리. GPT 연동 시 한국어→의도 추출 정확도 ↑

* **Q. 여러 로봇 동시에 제어?**
  A. `for all robots` 같은 그룹 명령은 Phase 5에 추가 예정.

* **Q. 실제 로봇 연결?**
  A. 시뮬레이터 → gRPC/WebSocket으로 ROS/마이크로컨트롤러에 명령 전달하는 어댑터 계층을 별도 모듈로 설계.

---

### Next Steps (당장 실행)

1. Vite 템플릿 생성 & 의존성 설치
2. 캔버스에 제공한 `PhysicalAIMVP` 컴포넌트 붙여 넣기
3. 명령 동작 확인 → `raise right arm`, `wave right arm`
4. Next.js API 라우트 `/api/intent`로 GPT 연동
5. 음성 입력 버튼 추가(Web Speech API)
